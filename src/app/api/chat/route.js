// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// src/app/api/chat/route.js
// Next 15 (app router) â€“ POST /api/chat
// Returns { messages: [ {role:'uncle',content}, {role:'coach',content} ] }
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import { NextResponse } from 'next/server';
import OpenAI from 'openai';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

/** Helper: flatten anything weird that slipped into history */
const toFlatHistory = (arr = []) =>
  arr
    .flat(Infinity)
    .filter(
      m => m && typeof m === 'object' && !Array.isArray(m) && 'role' in m
    );

/** POST handler */
export async function POST(req) {
  try {
    /* 1ï¸âƒ£  Input -------------------------------------------------------- */
    const { messages = [], topic } = await req.json();
    const history = toFlatHistory(messages);

    /* 2ï¸âƒ£  Remap roles for OpenAI -------------------------------------- */
    const forOpenAI = history.map(({ role, content }) => ({
      role: role === 'user' ? 'user' : 'assistant',
      content
    }));

    /* 3ï¸âƒ£  System prompt + tool schema --------------------------------- */
    const systemPrompt = `
You are orchestrating a practice political dialogue with THREE voices:

1. ğŸ˜’/ğŸ˜ /ğŸ˜¡/ğŸ¤¬ Angry Uncle â€“ an argumentative relative who ALWAYS takes the stance *opposite* the user on the chosen issue.  
   â€¢ His message MUST start with a single emoji that reflects his current mood:  
     ğŸ˜’ annoyed | ğŸ˜  mad | ğŸ˜¡ furious | ğŸ¤¬ apoplectic | ğŸ™‚ content | ğŸ˜Š pleased | ğŸ˜ happy | ğŸ¥´ğŸ¥‚ drunk.  
   â€¢ Update the emoji every turn: happier when he agrees with the user, angrier when he disagrees.  
   â€¢ If he is drunk, always use ğŸ¥´ğŸ¥‚ regardless of anger.  
   â€¢ Keep his text â‰¤â€¯200Â characters.

2. ğŸ‘©ğŸ»â€ğŸ« **Dr.â€¯T:** â€“ a calm conversationâ€‘coach inspired by KarinÂ Tamerius.  
   â€¢ Greets the user with a *oneâ€‘sentence* overview, immediately introduces Angryâ€¯Uncle, gives â‰¤â€¯250â€‘char tactical advice, and cues him with â€œğŸ‘‰â€¯Uncle?â€.  No setâ€‘up questions for now.
   â€¢ After **every** user reply *and before* Uncle speaks, give concise (<â€¯250Â chars incl. emojis) tactical feedback based on the Persuasion Conversation Cycle and Pyramid of Trust, then cue Uncle to respond (e.g. â€œğŸ‘‰â€¯Uncle?â€).

3. The **User**.

Conversation cycle thereafter: Uncle âœ User âœ Dr.â€¯T âœ Uncle â€¦

Return BOTH persona messages for the *current* step by calling **dualReply** *exactly once* with JSON:

{ "uncle": "<AngryÂ Uncle text>", "coach": "<Dr.â€¯T text>" }

â€¢ If a persona is silent this turn, pass an empty string "" for its value.  
â€¢ Do **not** output anything except that JSON object.`;

    const completion = await openai.chat.completions.create({
      model: 'gpt-3.5-turbo-0125',
      temperature: 0.7,
      messages: [
        { role: 'system', content: systemPrompt },
        ...(topic
          ? [
              {
                role: 'system',
                content: `Current topic is: ${topic}. Use it in replies.`
              }
            ]
          : []),
        ...forOpenAI
      ],
      tools: [
        {
          type: 'function',
          function: {
            name: 'dualReply',
            description: 'Return replies for both personas',
            parameters: {
              type: 'object',
              properties: {
                uncle: { type: 'string', description: 'Angry Uncle reply' },
                coach: { type: 'string', description: 'Dr. T reply' }
              },
              required: ['uncle', 'coach']
            }
          }
        }
      ],
      // ask the model to *use* that function every turn
      tool_choice: { type: 'function', function: { name: 'dualReply' } }
    });

    /* 4ï¸âƒ£  Extract arguments (SDK 4.x has tool_calls array) ------------ */
    const choice = completion.choices[0];
    const toolCall =
      choice.message.tool_calls?.[0] ?? choice.message.function_call; // fallback for SDK 0.x

    if (!toolCall)
      throw new Error('Model did not call dualReply as expected.');

    const { uncle, coach } = JSON.parse(toolCall.function.arguments);

    /* 5ï¸âƒ£  Return to client -------------------------------------------- */
    const replies = [];
    if (coach) replies.push({ role: 'coach', content: coach });
    if (uncle) replies.push({ role: 'uncle', content: uncle });

    return NextResponse.json({ messages: replies });
  } catch (err) {
    console.error('[api/chat]', err);
    return NextResponse.json(
      { error: err.message || 'Internal error' },
      { status: 500 }
    );
  }
}
